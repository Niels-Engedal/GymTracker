{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from scipy.interpolate import interp1d # for time interpolation\n",
    "\n",
    "from Sports2D import Sports2D\n",
    "\n",
    "# importing custom plotting functions\n",
    "from plotting_functions import plot_joint_angle, plot_all_joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def list_filepaths(directory, extension):\n",
    "    \"\"\"\n",
    "    Lists all *extension* files in the specified directory and its subdirectories.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): The path to the directory.\n",
    "    extension (str): The file extension to look for.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of file paths to *extension* files in the directory and subdirectories.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for file in filenames:\n",
    "            if file.endswith(str(extension)):\n",
    "                files.append(os.path.join(root, file))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing the video's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Processing ../Videos to Analyze/Kraft/kraft_good_id2_1.mp4\n",
      "On Wednesday 13. November 2024, 12:16:46\n",
      "---------------------------------------------------------------------\n",
      "  0%|          | 0/258 [00:00<?, ?it/s]\n",
      "Valid MPS installation found: using ONNXRuntime backend with GPU.\n",
      "\u001b[0;93m2024-11-13 12:16:47.362971 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:1475, shape: {400000,1}\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:16:47.363199 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 8 number of nodes in the graph: 415 number of nodes supported by CoreML: 378\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:16:50.930586 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:16:50.930600 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:16:50.978046 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 5 number of nodes in the graph: 247 number of nodes supported by CoreML: 204\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n",
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-11-13 12:16:52.342157 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:16:52.342169 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Pose tracking set up for BodyWithFeet model in balanced mode.\n",
      "Persons are detected every 1 frames and tracked inbetween. Multi-person is selected.\n",
      "Parameters: tracking_mode='sports2d', keypoint_likelihood_threshold=0.3, average_likelihood_threshold=0.5, keypoint_number_threshold=0.3\n",
      "\n",
      "Processing video stream...\n",
      "100%|██████████| 258/258 [00:18<00:00, 13.90it/s]\n",
      "Video processing completed.\n",
      "Processed video saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_good_id2_1_Sports2D/kraft_good_id2_1_Sports2D.mp4.\n",
      "\n",
      "Post-processing pose:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz.\n",
      "Pose saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_good_id2_1_Sports2D/kraft_good_id2_1_Sports2D_px_person00.trc.\n",
      "\n",
      "Post-processing angles:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz. \n",
      "Angles saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_good_id2_1_Sports2D/kraft_good_id2_1_Sports2D_angles_person00.mot.\n",
      "\n",
      "Processing ../Videos to Analyze/Kraft/kraft_good_id2_1.mp4 took 18.75 s.\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id3_1_nmspeed.mp4\n",
      "On Wednesday 13. November 2024, 12:17:04\n",
      "---------------------------------------------------------------------\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]\n",
      "Valid MPS installation found: using ONNXRuntime backend with GPU.\n",
      "\u001b[0;93m2024-11-13 12:17:05.088705 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:1475, shape: {400000,1}\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:05.088980 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 8 number of nodes in the graph: 415 number of nodes supported by CoreML: 378\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:08.341859 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:08.341873 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-11-13 12:17:08.366487 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 5 number of nodes in the graph: 247 number of nodes supported by CoreML: 204\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:09.706136 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:09.706150 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Pose tracking set up for BodyWithFeet model in balanced mode.\n",
      "Persons are detected every 1 frames and tracked inbetween. Multi-person is selected.\n",
      "Parameters: tracking_mode='sports2d', keypoint_likelihood_threshold=0.3, average_likelihood_threshold=0.5, keypoint_number_threshold=0.3\n",
      "\n",
      "Processing video stream...\n",
      "  1%|          | 1/84 [00:04<06:35,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:09<00:00,  9.20it/s]\n",
      "Video processing completed.\n",
      "Processed video saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id3_1_nmspeed_Sports2D/kraft_bad_id3_1_nmspeed_Sports2D.mp4.\n",
      "\n",
      "Post-processing pose:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz.\n",
      "Pose saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id3_1_nmspeed_Sports2D/kraft_bad_id3_1_nmspeed_Sports2D_px_person00.trc.\n",
      "\n",
      "Post-processing angles:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz. \n",
      "Angles saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id3_1_nmspeed_Sports2D/kraft_bad_id3_1_nmspeed_Sports2D_angles_person00.mot.\n",
      "\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id3_1_nmspeed.mp4 took 9.31 s.\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id5_1_nmspeed.mp4\n",
      "On Wednesday 13. November 2024, 12:17:14\n",
      "---------------------------------------------------------------------\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]\n",
      "Valid MPS installation found: using ONNXRuntime backend with GPU.\n",
      "\u001b[0;93m2024-11-13 12:17:14.455274 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:1475, shape: {400000,1}\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:14.455541 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 8 number of nodes in the graph: 415 number of nodes supported by CoreML: 378\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:17.666251 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:17.666262 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n",
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-11-13 12:17:17.682975 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 5 number of nodes in the graph: 247 number of nodes supported by CoreML: 204\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:18.988676 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:18.988688 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Pose tracking set up for BodyWithFeet model in balanced mode.\n",
      "Persons are detected every 1 frames and tracked inbetween. Multi-person is selected.\n",
      "Parameters: tracking_mode='sports2d', keypoint_likelihood_threshold=0.3, average_likelihood_threshold=0.5, keypoint_number_threshold=0.3\n",
      "\n",
      "Processing video stream...\n",
      "100%|██████████| 82/82 [00:08<00:00,  9.22it/s]\n",
      "Video processing completed.\n",
      "Processed video saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id5_1_nmspeed_Sports2D/kraft_bad_id5_1_nmspeed_Sports2D.mp4.\n",
      "\n",
      "Post-processing pose:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz.\n",
      "Pose saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id5_1_nmspeed_Sports2D/kraft_bad_id5_1_nmspeed_Sports2D_px_person00.trc.\n",
      "\n",
      "Post-processing angles:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz. \n",
      "Angles saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id5_1_nmspeed_Sports2D/kraft_bad_id5_1_nmspeed_Sports2D_angles_person00.mot.\n",
      "\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id5_1_nmspeed.mp4 took 9.09 s.\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id1_1_normal_speed.mp4\n",
      "On Wednesday 13. November 2024, 12:17:23\n",
      "---------------------------------------------------------------------\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]\n",
      "Valid MPS installation found: using ONNXRuntime backend with GPU.\n",
      "\u001b[0;93m2024-11-13 12:17:23.644923 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:1475, shape: {400000,1}\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:23.645172 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 8 number of nodes in the graph: 415 number of nodes supported by CoreML: 378\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:26.850090 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:26.850103 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n",
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-11-13 12:17:26.865751 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 5 number of nodes in the graph: 247 number of nodes supported by CoreML: 204\u001b[m\n",
      "Pose tracking set up for BodyWithFeet model in balanced mode.\n",
      "\u001b[0;93m2024-11-13 12:17:28.255225 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:28.255239 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Persons are detected every 1 frames and tracked inbetween. Multi-person is selected.\n",
      "Parameters: tracking_mode='sports2d', keypoint_likelihood_threshold=0.3, average_likelihood_threshold=0.5, keypoint_number_threshold=0.3\n",
      "\n",
      "Processing video stream...\n",
      "100%|██████████| 84/84 [00:09<00:00,  8.71it/s]\n",
      "Video processing completed.\n",
      "Processed video saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id1_1_normal_speed_Sports2D/kraft_bad_id1_1_normal_speed_Sports2D.mp4.\n",
      "\n",
      "Post-processing pose:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz.\n",
      "Pose saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id1_1_normal_speed_Sports2D/kraft_bad_id1_1_normal_speed_Sports2D_px_person00.trc.\n",
      "- Person 1: Less than 4 valid frames. Deleting person.\n",
      "\n",
      "Post-processing angles:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz. \n",
      "Angles saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id1_1_normal_speed_Sports2D/kraft_bad_id1_1_normal_speed_Sports2D_angles_person00.mot.\n",
      "- Person 1: Less than 4 valid frames. Deleting person.\n",
      "\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id1_1_normal_speed.mp4 took 9.85 s.\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id5_2_nmspeed.mp4\n",
      "On Wednesday 13. November 2024, 12:17:33\n",
      "---------------------------------------------------------------------\n",
      "  0%|          | 0/109 [00:00<?, ?it/s]\n",
      "Valid MPS installation found: using ONNXRuntime backend with GPU.\n",
      "\u001b[0;93m2024-11-13 12:17:33.578499 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:1475, shape: {400000,1}\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:33.578744 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 8 number of nodes in the graph: 415 number of nodes supported by CoreML: 378\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:36.794153 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:36.794165 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-11-13 12:17:36.818413 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 5 number of nodes in the graph: 247 number of nodes supported by CoreML: 204\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:38.092233 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:38.092243 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "Pose tracking set up for BodyWithFeet model in balanced mode.\n",
      "Persons are detected every 1 frames and tracked inbetween. Multi-person is selected.\n",
      "Parameters: tracking_mode='sports2d', keypoint_likelihood_threshold=0.3, average_likelihood_threshold=0.5, keypoint_number_threshold=0.3\n",
      "\n",
      "Processing video stream...\n",
      "  1%|          | 1/109 [00:04<08:23,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:10<00:00, 10.28it/s]\n",
      "Video processing completed.\n",
      "Processed video saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id5_2_nmspeed_Sports2D/kraft_bad_id5_2_nmspeed_Sports2D.mp4.\n",
      "\n",
      "Post-processing pose:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz.\n",
      "Pose saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id5_2_nmspeed_Sports2D/kraft_bad_id5_2_nmspeed_Sports2D_px_person00.trc.\n",
      "\n",
      "Post-processing angles:\n",
      "- Person 0: Interpolating missing sequences if they are smaller than 10 frames. Large gaps filled with last_value.\n",
      "Filtering with Butterworth filter, 4th order, 3 Hz. \n",
      "Angles saved to /Users/niels/Desktop/University/Third Semester/Perception and Action/Exam/Gymnastics Motion Tracking/Code for Gym Tracking/Scripts/Analyzed Data/kraft_bad_id5_2_nmspeed_Sports2D/kraft_bad_id5_2_nmspeed_Sports2D_angles_person00.mot.\n",
      "\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id5_2_nmspeed.mp4 took 10.79 s.\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Processing ../Videos to Analyze/Kraft/kraft_bad_id1_2_nmspeed.mp4\n",
      "On Wednesday 13. November 2024, 12:17:44\n",
      "---------------------------------------------------------------------\n",
      "  0%|          | 0/52 [00:00<?, ?it/s]\n",
      "Valid MPS installation found: using ONNXRuntime backend with GPU.\n",
      "\u001b[0;93m2024-11-13 12:17:44.440989 [W:onnxruntime:, helper.cc:82 IsInputSupported] CoreML does not support input dim > 16384. Input:1475, shape: {400000,1}\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:44.441211 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 8 number of nodes in the graph: 415 number of nodes supported by CoreML: 378\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:47.714772 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:47.714784 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:47.740185 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 5 number of nodes in the graph: 247 number of nodes supported by CoreML: 204\u001b[m\n",
      "Pose tracking set up for BodyWithFeet model in balanced mode.\n",
      "\u001b[0;93m2024-11-13 12:17:49.023281 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-13 12:17:49.023295 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n",
      "load /Users/niels/.cache/rtmlib/hub/checkpoints/rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Persons are detected every 1 frames and tracked inbetween. Multi-person is selected.\n",
      "Parameters: tracking_mode='sports2d', keypoint_likelihood_threshold=0.3, average_likelihood_threshold=0.5, keypoint_number_threshold=0.3\n",
      "\n",
      "Processing video stream...\n",
      " 96%|█████████▌| 50/52 [00:07<00:00, 19.27it/s]\u001b[1;31m2024-11-13 12:17:51.787977 [E:onnxruntime:, sequential_executor.cc:516 ExecuteKernel] Non-zero status code returned while running CoreML_13813013271624245768_5 node. Name:'CoreMLExecutionProvider_CoreML_13813013271624245768_5_5' Status Message: coreml_execution_provider.cc:200 operator() Input (1466) has a dynamic shape ({-1}) but the runtime shape ({0}) has zero elements. This is not supported by the CoreML EP.\u001b[m\n",
      " 98%|█████████▊| 51/52 [00:07<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing video ../Videos to Analyze/Kraft/kraft_bad_id1_2_nmspeed.mp4: [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running CoreML_13813013271624245768_5 node. Name:'CoreMLExecutionProvider_CoreML_13813013271624245768_5_5' Status Message: coreml_execution_provider.cc:200 operator() Input (1466) has a dynamic shape ({-1}) but the runtime shape ({0}) has zero elements. This is not supported by the CoreML EP.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from Sports2D import Sports2D\n",
    "\n",
    "# Define paths\n",
    "video_directory = \"../Videos to Analyze/Kraft\"\n",
    "config_path = \"../Configs/kraftspring_config_7nov.toml\"\n",
    "data_dir = \"../Analyzed Data\"\n",
    "\n",
    "# Get a list of all video files in the directory\n",
    "video_files = list_filepaths(video_directory, \".mp4\")\n",
    "\n",
    "# Load the configuration file\n",
    "config = Sports2D.read_config_file(config_path)\n",
    "\n",
    "# Update the video input in the config and process each video\n",
    "for video_file in video_files:\n",
    "    try:\n",
    "        config['project']['video_input'] = [video_file]\n",
    "        Sports2D.process(config)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trc_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads a .trc motion capture file into a Pandas DataFrame, handling multi-line headers and pairing marker names with coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the .trc file.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the header information (as a dict) and the data (as a DataFrame).\n",
    "    \"\"\"\n",
    "    # Read the entire file to access header lines\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    # Extract header information from the first three lines\n",
    "    headers = {\n",
    "        'PathFileType': lines[0].strip(),\n",
    "        'Data Info': lines[1].strip(),\n",
    "        'Rates and Frames': lines[2].strip()\n",
    "    }\n",
    "    \n",
    "    # Extract marker names and coordinate labels from the last two header lines\n",
    "    marker_names = lines[3].strip().split()  # e.g., ['Frame#', 'Time', 'Hip', 'RHip', ...]\n",
    "    coordinate_labels = lines[4].strip().split()  # e.g., ['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2', ...]\n",
    "\n",
    "    # Combine marker names with coordinate labels to form complete column names\n",
    "    column_names = []\n",
    "    marker_index = 2  # Start at 2 to skip 'Frame#' and 'Time'\n",
    "    for coord in coordinate_labels:\n",
    "        if coord.startswith('X'):\n",
    "            column_names.append(f\"{marker_names[marker_index]}_X\")\n",
    "        elif coord.startswith('Y'):\n",
    "            column_names.append(f\"{marker_names[marker_index]}_Y\")\n",
    "        elif coord.startswith('Z'):\n",
    "            column_names.append(f\"{marker_names[marker_index]}_Z\")\n",
    "            marker_index += 1\n",
    "\n",
    "    # Prepend 'Frame#' and 'Time' to the column names\n",
    "    column_names = ['Frame#', 'Time'] + column_names\n",
    "    \n",
    "    # Create the DataFrame starting from the data rows (after the header)\n",
    "    data_df = pd.read_csv(file_path, sep='\\s+', skiprows=5, names=column_names)\n",
    "    \n",
    "    return headers, data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mot_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads a .mot file containing joint angles into a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the .mot file.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing metadata (as a dict) and the data (as a DataFrame).\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    data_lines = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read and parse the header\n",
    "        line = file.readline().strip()\n",
    "        while line != \"endheader\":\n",
    "            if '=' in line:\n",
    "                key, value = line.split('=')\n",
    "                metadata[key.strip()] = value.strip()\n",
    "            line = file.readline().strip()\n",
    "        \n",
    "        # Read the column headers line\n",
    "        column_headers_line = file.readline().strip()\n",
    "        column_headers = column_headers_line.split('\\t')  # Split by tabs if columns are tab-separated\n",
    "        column_headers = [col.replace(' ', '_') for col in column_headers]  # Replace spaces in column names with underscores\n",
    "        \n",
    "        # Read the data lines\n",
    "        for line in file:\n",
    "            data_lines.append(line.strip().split('\\t'))  # Split by tabs if data is tab-separated\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    data_df = pd.DataFrame(data_lines, columns=column_headers)\n",
    "    \n",
    "    # Convert numerical columns from string to float\n",
    "    data_df = data_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    return metadata, data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_identifiers(file_path):\n",
    "    \"\"\"\n",
    "    Extracts move, evaluation, participant ID, video number, and person tracked from the file path.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the file.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing move, evaluation, participant ID, video number, and person tracked.\n",
    "    \"\"\"\n",
    "    # Regex to extract identifiers from the path after \"Sports2D/\"\n",
    "    match = re.search(r\"Sports2D/([^_]+)_(good|bad)_id(\\d+)_(\\d+)_.*_(person\\d+)\", file_path)\n",
    "    if match:\n",
    "        move, evaluation, participant_id, video_number, person_tracked = match.groups()\n",
    "        return move, evaluation, f\"id{participant_id}_{person_tracked}\", video_number\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot extract identifiers from the file path: {file_path}\")\n",
    "\n",
    "def load_multiple_files(file_paths, file_type='trc'):\n",
    "    \"\"\"\n",
    "    Loads multiple .trc or .mot files into a combined DataFrame with participant identifiers.\n",
    "\n",
    "    Parameters:\n",
    "    file_paths (list of str): List of paths to the .trc or .mot files.\n",
    "    file_type (str): Type of file to load ('trc' or 'mot').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A combined DataFrame with participant ID as a key and other metadata as columns.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            move, evaluation, participant_key, video_number = extract_identifiers(path)\n",
    "            \n",
    "            if file_type == 'trc':\n",
    "                _, data_df = load_trc_file(path)\n",
    "            elif file_type == 'mot':\n",
    "                metadata, data_df = load_mot_file(path)\n",
    "                # Optionally add metadata columns if needed\n",
    "                for key, value in metadata.items():\n",
    "                    data_df[key] = value\n",
    "            \n",
    "            # Add participant-specific information as new columns\n",
    "            data_df['participant_id'] = participant_key\n",
    "            data_df['move'] = move\n",
    "            data_df['evaluation'] = evaluation\n",
    "            data_df['video_number'] = video_number\n",
    "            \n",
    "            data_list.append(data_df)\n",
    "        \n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Combine all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(data_list, ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file paths contained in analyzed video data\n",
    "trc_file_paths = list_filepaths(data_dir, '.trc')\n",
    "mot_file_paths = list_filepaths(data_dir, '.mot')\n",
    "\n",
    "# create combined dataframes of them\n",
    "trc_combined_df = load_multiple_files(trc_file_paths, file_type='trc')\n",
    "mot_combined_df = load_multiple_files(mot_file_paths, file_type='mot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique participants in mot_files ['id2_person00' 'id5_person00' 'id3_person00' 'id1_person00']\n",
      "Unique participants in trc_files ['id2_person00' 'id5_person00' 'id3_person00' 'id1_person00']\n",
      "\n",
      "Unique moves in mot_files ['kraft']\n",
      "Unique moves in trc_files ['kraft']\n",
      "\n",
      "Unique evaluations in mot_files ['good' 'bad']\n",
      "Unique evaluations in trc_files ['good' 'bad']\n",
      "\n",
      "Unique video numbers in mot_files ['1' '2']\n",
      "Unique video numbers in trc_files ['1' '2']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique participants in mot_files {mot_combined_df['participant_id'].unique()}\")\n",
    "print(f\"Unique participants in trc_files {trc_combined_df['participant_id'].unique()}\")\n",
    "\n",
    "print()\n",
    "print(f\"Unique moves in mot_files {mot_combined_df['move'].unique()}\")\n",
    "print(f\"Unique moves in trc_files {trc_combined_df['move'].unique()}\")\n",
    "\n",
    "print()\n",
    "print(f\"Unique evaluations in mot_files {mot_combined_df['evaluation'].unique()}\")\n",
    "print(f\"Unique evaluations in trc_files {trc_combined_df['evaluation'].unique()}\")\n",
    "\n",
    "print()\n",
    "print(f\"Unique video numbers in mot_files {mot_combined_df['video_number'].unique()}\")\n",
    "print(f\"Unique video numbers in trc_files {trc_combined_df['video_number'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>right_knee</th>\n",
       "      <th>left_knee</th>\n",
       "      <th>left_hip</th>\n",
       "      <th>right_shoulder</th>\n",
       "      <th>left_shoulder</th>\n",
       "      <th>right_elbow</th>\n",
       "      <th>left_elbow</th>\n",
       "      <th>version</th>\n",
       "      <th>nRows</th>\n",
       "      <th>nColumns</th>\n",
       "      <th>inDegrees</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>move</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>video_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.097969</td>\n",
       "      <td>7.277865</td>\n",
       "      <td>68.480268</td>\n",
       "      <td>174.192347</td>\n",
       "      <td>114.094754</td>\n",
       "      <td>-38.207582</td>\n",
       "      <td>6.191491</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>7</td>\n",
       "      <td>yes</td>\n",
       "      <td>id2_person00</td>\n",
       "      <td>kraft</td>\n",
       "      <td>good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006971</td>\n",
       "      <td>101.122264</td>\n",
       "      <td>7.238547</td>\n",
       "      <td>68.518884</td>\n",
       "      <td>174.222379</td>\n",
       "      <td>114.053705</td>\n",
       "      <td>-38.263336</td>\n",
       "      <td>6.183936</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>7</td>\n",
       "      <td>yes</td>\n",
       "      <td>id2_person00</td>\n",
       "      <td>kraft</td>\n",
       "      <td>good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013943</td>\n",
       "      <td>101.150709</td>\n",
       "      <td>7.195587</td>\n",
       "      <td>68.543118</td>\n",
       "      <td>174.240329</td>\n",
       "      <td>114.016246</td>\n",
       "      <td>-38.298380</td>\n",
       "      <td>6.175673</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>7</td>\n",
       "      <td>yes</td>\n",
       "      <td>id2_person00</td>\n",
       "      <td>kraft</td>\n",
       "      <td>good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020914</td>\n",
       "      <td>101.183615</td>\n",
       "      <td>7.149052</td>\n",
       "      <td>68.549590</td>\n",
       "      <td>174.243401</td>\n",
       "      <td>113.983813</td>\n",
       "      <td>-38.307839</td>\n",
       "      <td>6.166714</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>7</td>\n",
       "      <td>yes</td>\n",
       "      <td>id2_person00</td>\n",
       "      <td>kraft</td>\n",
       "      <td>good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027886</td>\n",
       "      <td>101.221266</td>\n",
       "      <td>7.099094</td>\n",
       "      <td>68.534564</td>\n",
       "      <td>174.228512</td>\n",
       "      <td>113.958039</td>\n",
       "      <td>-38.286334</td>\n",
       "      <td>6.157085</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>7</td>\n",
       "      <td>yes</td>\n",
       "      <td>id2_person00</td>\n",
       "      <td>kraft</td>\n",
       "      <td>good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  right_knee  left_knee   left_hip  right_shoulder  left_shoulder  \\\n",
       "0  0.000000  101.097969   7.277865  68.480268      174.192347     114.094754   \n",
       "1  0.006971  101.122264   7.238547  68.518884      174.222379     114.053705   \n",
       "2  0.013943  101.150709   7.195587  68.543118      174.240329     114.016246   \n",
       "3  0.020914  101.183615   7.149052  68.549590      174.243401     113.983813   \n",
       "4  0.027886  101.221266   7.099094  68.534564      174.228512     113.958039   \n",
       "\n",
       "   right_elbow  left_elbow version nRows nColumns inDegrees participant_id  \\\n",
       "0   -38.207582    6.191491       1   258        7       yes   id2_person00   \n",
       "1   -38.263336    6.183936       1   258        7       yes   id2_person00   \n",
       "2   -38.298380    6.175673       1   258        7       yes   id2_person00   \n",
       "3   -38.307839    6.166714       1   258        7       yes   id2_person00   \n",
       "4   -38.286334    6.157085       1   258        7       yes   id2_person00   \n",
       "\n",
       "    move evaluation video_number  \n",
       "0  kraft       good            1  \n",
       "1  kraft       good            1  \n",
       "2  kraft       good            1  \n",
       "3  kraft       good            1  \n",
       "4  kraft       good            1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot_combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hip_trajectory(df):\n",
    "    \"\"\"\n",
    "    Plots the hip position (X and Y) for multiple participants from a combined DataFrame,\n",
    "    distinguishing between 'good' and 'bad' participants by line style.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): A DataFrame containing motion capture data with columns 'Hip_X', 'Hip_Y',\n",
    "                       'participant_id', and 'evaluation'.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Generate a color map for unique participants\n",
    "    unique_participants = df['participant_id'].unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_participants)))\n",
    "    color_map = {participant: color for participant, color in zip(unique_participants, colors)}\n",
    "    \n",
    "    # Plot hip trajectory for each participant\n",
    "    for participant_id in unique_participants:\n",
    "        participant_data = df[df['participant_id'] == participant_id]\n",
    "        if 'Hip_X' in participant_data.columns and 'Hip_Y' in participant_data.columns:\n",
    "            evaluation = participant_data['evaluation'].iloc[0]  # Get the evaluation value for the participant\n",
    "            line_style = '-' if evaluation == 'good' else '--'\n",
    "            plt.plot(participant_data['Hip_X'], participant_data['Hip_Y'],\n",
    "                     label=f'{evaluation} - {participant_id}',\n",
    "                     color=color_map[participant_id], linestyle=line_style)\n",
    "        else:\n",
    "            print(f\"Warning: 'Hip_X' or 'Hip_Y' not found for {participant_id}\")\n",
    "    \n",
    "    plt.xlabel('X Position (m)')\n",
    "    plt.ylabel('Y Position (m)')\n",
    "    plt.title('Hip Position in X-Y Plane Over Time for Multiple Participants')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_trc_df and combined_mot_df are DataFrames with motion capture data\n",
    "#plot_hip_trajectory(trc_combined_df)\n",
    "#standardized_trc_df = standardize_trc_data(trc_combined_df)\n",
    "plot_joint_angle(mot_combined_df, 'right_knee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with a DataFrame containing joint data\n",
    "joint_names = ['right_knee', 'right_elbow']  # List of joints to plot\n",
    "plot_all_joints(mot_combined_df, joint_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GymSports2D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
